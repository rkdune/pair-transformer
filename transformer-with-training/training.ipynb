{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257e14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np  # reserved for later use\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a57cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8830dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 50257\n",
    "        self.embedding_dim = 1024\n",
    "        self.mlp_dim = 4 * self.embedding_dim\n",
    "        self.num_blocks = 4\n",
    "        self.num_heads = 8\n",
    "        self.context_len = 1024\n",
    "        self.attention_dim = self.embedding_dim // self.num_heads\n",
    "        # mps (Apple Silicon) support, reserved for further training\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f93a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T):\n",
    "        self.batch_size = B\n",
    "        self.seq_len = T\n",
    "    \n",
    "        with open(\"tiny_shakespeare_dataset.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "\n",
    "        self.current_pos = 0 # maintain the index of the current data sample\n",
    "\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"each epoch has {len(self.tokens) / len(self.batch_size * self.seq_len)} \")\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.batch_size, self.seq_len\n",
    "        x = self.tokens[self.current_position:self.current_position+BT]\n",
    "        y = self.tokens[self.current_position+1:self.current_position+BT+1]\n",
    "        x = x.view(B, T)\n",
    "        y = y.view(B, T)\n",
    "        self.current_position += B * T\n",
    "\n",
    "        if (len(self.tokens) - self.current_position + 1) < B * T:\n",
    "            self.current_position = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6078d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.W_pos = nn.Embedding(config.context_len, config.embedding_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        tokens = torch.tensor(tokens)\n",
    "        embeddings = self.W_E(tokens)\n",
    "\n",
    "        # print(f\"---------------------------{tokens.shape}\")\n",
    "        positions = self.W_pos(torch.arange(tokens.shape[1]))\n",
    "\n",
    "        return embeddings + positions\n",
    "\n",
    "class DeEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_D = nn.Linear(config.embedding_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.W_D(x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f447d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_K = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_V = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        \n",
    "        self.W_out = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attention_dim = config.attention_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = einops.rearrange(self.W_Q(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        K = einops.rearrange(self.W_K(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        V = einops.rearrange(self.W_V(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "\n",
    "        QK = torch.softmax((Q @ K.transpose(-2, -1))/(torch.sqrt(torch.tensor(self.attention_dim))), dim = -1)\n",
    "\n",
    "        QKV = einops.rearrange(QK @ V, 'batch head seq dim -> batch seq (head dim)', head=config.num_heads)\n",
    "        QKV_Out = self.W_out(QKV)\n",
    "\n",
    "        return QKV_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2142239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(config.embedding_dim, config.mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(config.mlp_dim, config.embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer2(self.gelu(self.layer1(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154e6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.MLP_Layers = MLP(config)\n",
    "        self.Attention_Layers = Attention(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.Attention_Layers(x)\n",
    "        return x + self.MLP_Layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d97092ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embedding(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for i in range(config.num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.deembed = DeEmbedding(config)\n",
    "        # self.deembed.W_D.weight = self.embed.W_E.weight  # tie weights\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        # print(f\"after embed: {x}\")\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.deembed(x)\n",
    "        # print(f\"after deembed: {x}\")\n",
    "        x = torch.softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db70dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: tensor([[39806, 44034, 26845,  8633]])\n",
      "predicted word:  dict\n",
      "full sentence: The quick brown fox dict\n",
      "sanity check: all predicted tokens\n",
      "Token 39806 -> 'blast'\n",
      "Token 44034 -> 'avement'\n",
      "Token 26845 -> 'articles'\n",
      "Token 8633 -> ' dict'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/xngj7qq57vg9mrvtkf3kzjyc0000gn/T/ipykernel_82112/1852530313.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    text = \"The quick brown fox\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    x = torch.tensor(tokens)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # print(f\"---------------{x.shape}\")\n",
    "\n",
    "    model = Transformer(config)\n",
    "\n",
    "    out = model(x)\n",
    "    pred_tokens = out.argmax(dim=-1)\n",
    "    print(f\"predicted tokens: {pred_tokens}\")\n",
    "\n",
    "    # Only take the prediction from the last position (next token after \"fox\")\n",
    "    next_token = pred_tokens[0, -1].item()\n",
    "    predicted_word = tokenizer.decode([next_token])\n",
    "    print(f\"predicted word: {predicted_word}\")\n",
    "\n",
    "    print(f\"full sentence: {text}{predicted_word}\")\n",
    "\n",
    "    print(\"sanity check: all predicted tokens\")\n",
    "    for token in pred_tokens.flatten():\n",
    "        decoded = tokenizer.decode([token.item()])\n",
    "        print(f\"Token {token} -> '{decoded}'\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## TODOs\n",
    "# \n",
    "# - [ ] Add support for displaying calculated # of learnable and non-learnable params\n",
    "# - [ ] Attention sink?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
