{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "257e14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np  # reserved for later use\n",
    "import einops\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "5a57cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "8830dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 50257\n",
    "        self.embedding_dim = 1024\n",
    "        self.mlp_dim = 4 * self.embedding_dim\n",
    "        self.num_blocks = 4\n",
    "        self.num_heads = 8\n",
    "        self.context_len = 1024\n",
    "        self.attention_dim = self.embedding_dim // self.num_heads\n",
    "        # mps (Apple Silicon) support, reserved for further training\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "        # hyperparams\n",
    "        self.lr = 3e-4\n",
    "        self.betas = (0.9, 0.95) # for controlling momentum\n",
    "        self.eps = 1e-8\n",
    "        self.weight_decay = 0.1\n",
    "\n",
    "        self.learnable_params_dict = {\"embedding\": self.vocab_size * self.embedding_dim, \"positional_embedding\": self.context_len * self.embedding_dim, \"MLPs (Weights)\": self.num_blocks * 2 * self.embedding_dim * self.mlp_dim, \"MLPs (Biases)\": self.num_blocks * (self.mlp_dim + self.embedding_dim), \"W_Qs\": self.num_blocks * self.embedding_dim * self.embedding_dim, \"W_Ks\": self.num_blocks * self.embedding_dim * self.embedding_dim, \"W_Vs\": self.num_blocks * self.embedding_dim * self.embedding_dim, \"W_Out\": self.num_blocks * self.embedding_dim * self.embedding_dim}\n",
    "        self.learnable_params = (lambda d: sum(d.values()))(self.learnable_params_dict)\n",
    "\n",
    "        self.non_learnable_params_dict = {\"deembedding (tied to embedding weights)\": self.vocab_size * self.embedding_dim}\n",
    "        self.non_learnable_params = (lambda d: sum(d.values()))(self.non_learnable_params_dict)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "a7f93a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T):\n",
    "        self.batch_size = B # num of sequences processed together in each batch\n",
    "        self.seq_len = T # how many tokens are in each sequence/batch\n",
    "    \n",
    "        with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = torch.tensor(encoding)\n",
    "\n",
    "        self.current_pos = 0 # maintain the index of the current data sample\n",
    "\n",
    "        print(f\"loaded {len(self.tokens)} tokens with batch size of {self.batch_size} sequences and {self.seq_len} tokens per sequence in the batch\")\n",
    "        print(f\"each epoch has {len(self.tokens) / (self.batch_size * self.seq_len)} batches, with {self.seq_len * self.batch_size} tokens per batch, for a total of {self.seq_len * self.batch_size * (len(self.tokens) / (self.batch_size * self.seq_len))} tokens\")\n",
    "        print(\"*\"*50)\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.batch_size, self.seq_len\n",
    "        x = self.tokens[self.current_pos:self.current_pos+B*T]\n",
    "        y = self.tokens[self.current_pos+1:self.current_pos+B*T+1]\n",
    "        x = x.view(B, T)\n",
    "        y = y.view(B, T)\n",
    "        self.current_pos += B * T\n",
    "\n",
    "        if (len(self.tokens) - self.current_pos + 1) < B * T:\n",
    "            self.current_pos = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "6078d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.W_pos = nn.Embedding(config.context_len, config.embedding_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # Don't convert to tensor if already a tensor\n",
    "        if not isinstance(tokens, torch.Tensor):\n",
    "            tokens = torch.tensor(tokens)\n",
    "        \n",
    "        embeddings = self.W_E(tokens)\n",
    "\n",
    "        # Create positions tensor on the same device as tokens\n",
    "        positions = torch.arange(tokens.shape[1], device=tokens.device)\n",
    "        position_embeddings = self.W_pos(positions)\n",
    "\n",
    "        return embeddings + position_embeddings\n",
    "\n",
    "class DeEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_D = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.W_D(x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "42f447d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_K = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_V = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        \n",
    "        self.W_out = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attention_dim = config.attention_dim\n",
    "        \n",
    "        # Register causal mask as buffer\n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.context_len, config.context_len)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        Q = einops.rearrange(self.W_Q(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        K = einops.rearrange(self.W_K(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        V = einops.rearrange(self.W_V(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        scores = (Q @ K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.attention_dim))\n",
    "        \n",
    "        # Apply causal mask\n",
    "        scores = scores.masked_fill(self.causal_mask[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        QK = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        QKV = einops.rearrange(QK @ V, 'batch head seq dim -> batch seq (head dim)', head=self.num_heads)\n",
    "        QKV_Out = self.W_out(QKV)\n",
    "\n",
    "        return QKV_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "2142239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(config.embedding_dim, config.mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(config.mlp_dim, config.embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer2(self.gelu(self.layer1(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "154e6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Attention_Layers = Attention(config)\n",
    "        self.MLP_Layers = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.Attention_Layers(x)\n",
    "        return x + self.MLP_Layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "d97092ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embedding(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for i in range(config.num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.deembed = DeEmbedding(config)\n",
    "        self.deembed.W_D.weight = self.embed.W_E.weight  # tie weights\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        # print(f\"after embed: {x}\")\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.deembed(x)\n",
    "        # print(f\"after deembed: {x}\")\n",
    "        x = torch.softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "2db70dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnable params dict: {'embedding': 51463168, 'positional_embedding': 1048576, 'MLPs (Weights)': 33554432, 'MLPs (Biases)': 20480, 'W_Qs': 4194304, 'W_Ks': 4194304, 'W_Vs': 4194304, 'W_Out': 4194304}\n",
      "total # of learnable params: 102,863,872\n",
      "non-learnable params dict: {'deembedding (tied to embedding weights)': 51463168}\n",
      "total # of non-learnable params: 51,463,168\n",
      "** total # of params: 154,327,040 **\n",
      "Using device: mps\n",
      "loaded 338025 tokens with batch size of 8 sequences and 1024 tokens per sequence in the batch\n",
      "each epoch has 41.2628173828125 batches, with 8192 tokens per batch, for a total of 338025.0 tokens\n",
      "**************************************************\n",
      "using passed in model for inference\n",
      "predicted tokens: tensor([[2990, 3252,  514]], device='mps:0')\n",
      "predicted word:  us\n",
      "full sentence: They fear us us\n",
      "**************************************************\n",
      "sanity check: all predicted tokens\n",
      "Token 2990 -> 'They'\n",
      "Token 3252 -> ' fear'\n",
      "** Token 514 -> ' us' **\n"
     ]
    }
   ],
   "source": [
    "def inference(inference_config, inference_model):\n",
    "    text = \"They fear us\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    x = torch.tensor(tokens)\n",
    "    x = x.unsqueeze(0)\n",
    "    \n",
    "    # Move input tensor to the same device as the model\n",
    "    if inference_model:\n",
    "        print(\"using passed in model for inference\")\n",
    "        device = next(inference_model.parameters()).device\n",
    "        x = x.to(device)\n",
    "    else:\n",
    "        inference_model = Transformer(inference_config)\n",
    "        print(\"using random model for inference\")\n",
    "        device = inference_config.device\n",
    "        x = x.to(device)\n",
    "\n",
    "    out = inference_model(x)\n",
    "    pred_tokens = out.argmax(dim=-1)\n",
    "    print(f\"predicted tokens: {pred_tokens}\")\n",
    "\n",
    "    # Only take the prediction from the last position (next token after \"fox\")\n",
    "    next_token = pred_tokens[0, -1].item()\n",
    "    predicted_word = tokenizer.decode([next_token])\n",
    "    print(f\"predicted word: {predicted_word}\")\n",
    "\n",
    "    print(f\"full sentence: {text}{predicted_word}\")\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    print(\"sanity check: all predicted tokens\")\n",
    "    for num, token in enumerate(pred_tokens.flatten()):\n",
    "        decoded = tokenizer.decode([token.item()])\n",
    "        \n",
    "        if num == (len(pred_tokens.flatten()) - 1):\n",
    "            print(f\"** Token {token} -> '{decoded}' **\")\n",
    "        else:\n",
    "            print(f\"Token {token} -> '{decoded}'\")\n",
    "\n",
    "def training(model_config):\n",
    "    device = model_config.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.manual_seed(42)\n",
    "\n",
    "    train_loader = DataLoader(8, 1024)\n",
    "\n",
    "    model = Transformer(model_config).to(device)\n",
    "    # model = torch.compile(model) # temporary comment to resolve errors with metal\n",
    "    losses = []\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8, weight_decay = 0.1)\n",
    "\n",
    "    return model\n",
    "\n",
    "def display_params_info():\n",
    "    print(f\"learnable params dict: {config.learnable_params_dict}\")\n",
    "    print(f\"total # of learnable params: {config.learnable_params:,}\")\n",
    "    print(f\"non-learnable params dict: {config.non_learnable_params_dict}\")\n",
    "    print(f\"total # of non-learnable params: {config.non_learnable_params:,}\")\n",
    "    print(f\"** total # of params: {(config.learnable_params + config.non_learnable_params):,} **\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_params_info()\n",
    "    model = training(config)\n",
    "    inference(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "09dfc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## TODOs\n",
    "# - [x] working training\n",
    "# - [ ] Add support for displaying calculated # of learnable and non-learnable params\n",
    "# - [ ] Attention sink?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
