{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "257e14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import webbrowser\n",
    "import gdown\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from transformer_lens.utils import get_corner\n",
    "import circuitsvis as cv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a57cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8830dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 50257\n",
    "        self.embedding_dim = 1024\n",
    "        self.mlp_dim = 4 * self.embedding_dim\n",
    "        self.num_blocks = 4\n",
    "        self.num_heads = 8\n",
    "        self.context_len = 1024\n",
    "        self.attention_dim = self.embedding_dim // self.num_heads\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.dtype = torch.bfloat16\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6078d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_E= nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.W_pos = nn.Embedding(config.context_len, config.embedding_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        tokens = torch.tensor(tokens)\n",
    "        embeddings = self.W_E(tokens)\n",
    "\n",
    "        print(f\"---------------------------{tokens.shape}\")\n",
    "        positions = self.W_pos(torch.arange(tokens.shape[1]))\n",
    "\n",
    "        return embeddings + positions\n",
    "\n",
    "class DeEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_D = nn.Linear(config.embedding_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.W_D(x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42f447d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_K = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_V = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        \n",
    "        self.W_out = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attention_dim = config.attention_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = einops.rearrange(self.W_Q(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        K = einops.rearrange(self.W_K(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        V = einops.rearrange(self.W_V(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "\n",
    "        QK = torch.softmax((Q @ K.transpose(-2, -1))/(torch.sqrt(torch.tensor(self.attention_dim))), dim = -1)\n",
    "\n",
    "        QKV = einops.rearrange(QK @ V, 'batch head seq dim -> batch seq (head dim)', head=config.num_heads)\n",
    "        QKV_Out = self.W_out(QKV)\n",
    "\n",
    "        return QKV_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2142239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(config.embedding_dim, config.mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(config.mlp_dim, config.embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.softmax(self.layer2(self.gelu(self.layer1(x))), dim = -1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "154e6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.MLP_Layers = MLP(config)\n",
    "        self.Attention_Layers = Attention(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.MLP_Layers(x + self.Attention_Layers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d97092ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embedding(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for i in range(config.num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.deembed = DeEmbedding(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        print(f\"after embed: {x}\")\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.deembed(x)\n",
    "        print(f\"after deembed: {x}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2db70dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2068, 7586, 21831]\n",
      "---------------torch.Size([1, 4])\n",
      "---------------------------torch.Size([1, 4])\n",
      "after embed: tensor([[[ 3.6693,  2.1101,  0.6655,  ..., -1.3162,  0.7272,  2.2976],\n",
      "         [-0.5886, -1.8766,  0.6572,  ..., -0.9764,  2.0133, -1.8102],\n",
      "         [ 1.7779,  0.8118, -2.2432,  ..., -1.3450,  1.0963, -2.1877],\n",
      "         [ 0.0666,  0.6621,  0.1826,  ..., -2.3854,  0.6116,  1.1658]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after deembed: tensor([[[ 1.1262, -1.5290,  0.6249,  ...,  0.5261, -1.1176, -0.2258],\n",
      "         [-1.5563,  0.0773,  1.0925,  ..., -0.3508,  0.4950,  0.0473],\n",
      "         [-0.4413, -0.1252, -0.6700,  ..., -0.2989, -1.1514, -0.5644],\n",
      "         [ 0.6584,  0.4800, -1.2056,  ..., -0.8050, -0.8617, -0.3772]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "The quick brown foxrefused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/xngj7qq57vg9mrvtkf3kzjyc0000gn/T/ipykernel_91891/186248610.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    text = \"The quick brown fox\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(tokens)\n",
    "\n",
    "    x = torch.tensor(tokens)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    print(f\"---------------{x.shape}\")\n",
    "\n",
    "    model = Transformer(config)\n",
    "\n",
    "    out = model(x)\n",
    "    pred_tokens = out.argmax(dim=-1)\n",
    "\n",
    "    token_list = pred_tokens[0].tolist()\n",
    "    decoded_text = tokenizer.decode(token_list)\n",
    "\n",
    "    print(text + decoded_text.split()[-1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
