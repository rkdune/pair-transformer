{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "257e14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np  # reserved for later use\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a57cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 50257\n",
    "        self.embedding_dim = 1024\n",
    "        self.mlp_dim = 4 * self.embedding_dim\n",
    "        self.num_blocks = 4\n",
    "        self.num_heads = 8\n",
    "        self.context_len = 1024\n",
    "        self.attention_dim = self.embedding_dim // self.num_heads\n",
    "        # mps (Apple Silicon) support, reserved for further training\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.W_pos = nn.Embedding(config.context_len, config.embedding_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        tokens = torch.tensor(tokens)\n",
    "        embeddings = self.W_E(tokens)\n",
    "\n",
    "        # print(f\"---------------------------{tokens.shape}\")\n",
    "        positions = self.W_pos(torch.arange(tokens.shape[1]))\n",
    "\n",
    "        return embeddings + positions\n",
    "\n",
    "class DeEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_D = nn.Linear(config.embedding_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.W_D(x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42f447d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_K = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_V = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        \n",
    "        self.W_out = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attention_dim = config.attention_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = einops.rearrange(self.W_Q(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        K = einops.rearrange(self.W_K(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        V = einops.rearrange(self.W_V(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "\n",
    "        QK = torch.softmax((Q @ K.transpose(-2, -1))/(torch.sqrt(torch.tensor(self.attention_dim))), dim = -1)\n",
    "\n",
    "        QKV = einops.rearrange(QK @ V, 'batch head seq dim -> batch seq (head dim)', head=config.num_heads)\n",
    "        QKV_Out = self.W_out(QKV)\n",
    "\n",
    "        return QKV_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2142239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(config.embedding_dim, config.mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(config.mlp_dim, config.embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer2(self.gelu(self.layer1(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.MLP_Layers = MLP(config)\n",
    "        self.Attention_Layers = Attention(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.Attention_Layers(x)\n",
    "        return x + self.MLP_Layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97092ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embedding(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for i in range(config.num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.deembed = DeEmbedding(config)\n",
    "        # self.deembed.W_D.weight = self.embed.W_E.weight  # tie weights\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        # print(f\"after embed: {x}\")\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.deembed(x)\n",
    "        # print(f\"after deembed: {x}\")\n",
    "        x = torch.softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2db70dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens: tensor([[  464,  2068,  7586, 21831]])\n",
      "predicted word: fox\n",
      "full sentence: The quick brown fox fox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8q/x_mr5vfd2c9csvhcv_yn13v40000gn/T/ipykernel_23951/2849814653.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    text = \"The quick brown fox\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    x = torch.tensor(tokens)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # print(f\"---------------{x.shape}\")\n",
    "\n",
    "    model = Transformer(config)\n",
    "\n",
    "    out = model(x)\n",
    "    pred_tokens = out.argmax(dim=-1)\n",
    "    print(f\"predicted tokens: {pred_tokens}\")  \n",
    "\n",
    "    token_list = pred_tokens[-1].tolist()\n",
    "    decoded_text = tokenizer.decode(token_list)\n",
    "    print(f\"predicted word: {decoded_text.split()[-1]}\")\n",
    "\n",
    "    print(f\"full sentence: {text} {decoded_text.split()[-1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856f3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
