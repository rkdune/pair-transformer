{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "257e14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np  # reserved for later use\n",
    "import einops\n",
    "import time\n",
    "import wandb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from muon import SingleDeviceMuonWithAuxAdam\n",
    "    MUON_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MUON_AVAILABLE = False\n",
    "    print(\"Muon not available. Install with: pip install git+https://github.com/KellerJordan/Muon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5a57cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8830dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    # FIXED PARAMS\n",
    "    vocab_size = 50257  # tiktoken vocab size for gpt2\n",
    "    \n",
    "    # CHANGEABLE MODEL ARCHITECTURE PARAMS\n",
    "    embedding_dim = 720\n",
    "    num_blocks = 2\n",
    "    num_heads = 2\n",
    "    context_len = 1024\n",
    "    \n",
    "    # TRAINING HYPERPARAMS\n",
    "    num_epochs = 1  # generally should keep this to 1\n",
    "    lr = 3e-4\n",
    "    muon_lr = 0.02 # from muon repo: \"only the lr and weight decay have to be tuned\"\n",
    "    muon_momentum = 0.95\n",
    "    betas = (0.9, 0.95)  # for controlling momentum\n",
    "    eps = 1e-8\n",
    "    weight_decay = 0.01\n",
    "    use_muon = True  # whether to use Muon optimizer for hidden layers\n",
    "    \n",
    "    # LOGGING & OBSERVABILITY\n",
    "    wandb_enabled = True\n",
    "    print_per_layer_params = False\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # Set all class attributes as instance attributes first\n",
    "        for key in dir(self.__class__):\n",
    "            if not key.startswith('_') and not callable(getattr(self.__class__, key)):\n",
    "                setattr(self, key, getattr(self.__class__, key))\n",
    "        \n",
    "        # Override with any provided kwargs\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self.__class__, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown config parameter: {key}\")\n",
    "        \n",
    "        # Derived params that depend on other params\n",
    "        self.mlp_dim = 2 * self.embedding_dim\n",
    "        self.attention_dim = self.embedding_dim // self.num_heads\n",
    "        \n",
    "        # Device detection\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "        # COUNTING PARAMETERS\n",
    "        # learnable params\n",
    "        self.learnable_params_dict = {\n",
    "            \"embedding\": self.vocab_size * self.embedding_dim, \n",
    "            \"positional_embedding\": self.context_len * self.embedding_dim, \n",
    "            \"MLPs (Weights)\": self.num_blocks * 2 * self.embedding_dim * self.mlp_dim, \n",
    "            \"MLPs (Biases)\": self.num_blocks * (self.mlp_dim + self.embedding_dim), \n",
    "            \"W_Qs\": self.num_blocks * self.embedding_dim * self.embedding_dim, \n",
    "            \"W_Ks\": self.num_blocks * self.embedding_dim * self.embedding_dim, \n",
    "            \"W_Vs\": self.num_blocks * self.embedding_dim * self.embedding_dim, \n",
    "            \"W_Out\": self.num_blocks * self.embedding_dim * self.embedding_dim}\n",
    "        self.learnable_params = (lambda d: sum(d.values()))(self.learnable_params_dict)\n",
    "\n",
    "        # non-learnable (fixed) params\n",
    "        self.non_learnable_params_dict = {\"deembedding (tied to embedding weights)\": self.vocab_size * self.embedding_dim}\n",
    "        self.non_learnable_params = (lambda d: sum(d.values()))(self.non_learnable_params_dict)\n",
    "    \n",
    "    def display_config(self, extended):\n",
    "        if extended:\n",
    "            print(f\"learnable params dict: {config.learnable_params_dict}\")\n",
    "            print(f\"total # of learnable params: {config.learnable_params:,}\")\n",
    "            print(f\"non-learnable params dict: {config.non_learnable_params_dict}\")\n",
    "            print(f\"total # of non-learnable params: {config.non_learnable_params:,}\")\n",
    "            print(f\"** total # of params: {(config.learnable_params + config.non_learnable_params):,} **\")\n",
    "            print(\"*\"*50)\n",
    "        else:\n",
    "            print(f\"learnable params: {self.learnable_params:,}, non-learnable params: {self.non_learnable_params:,}, total params: {self.learnable_params + self.non_learnable_params:,}\")\n",
    "\n",
    "# config = Config()\n",
    "# config.display_config(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a7f93a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T):\n",
    "        self.batch_size = B # num of sequences processed together in each batch\n",
    "        self.seq_len = T # how many tokens are in each sequence/batch\n",
    "    \n",
    "        with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = torch.tensor(encoding)\n",
    "\n",
    "        self.current_pos = 0 # maintain the index of the current data sample\n",
    "\n",
    "        print(f\"loaded {len(self.tokens)} tokens with batch size of {self.batch_size} sequences and {self.seq_len} tokens per sequence in the batch\")\n",
    "        print(f\"each epoch has {len(self.tokens) / (self.batch_size * self.seq_len)} batches, with {self.seq_len * self.batch_size} tokens per batch, for a total of {self.seq_len * self.batch_size * (len(self.tokens) / (self.batch_size * self.seq_len))} tokens\")\n",
    "        print(\"*\"*50)\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.batch_size, self.seq_len\n",
    "        x = self.tokens[self.current_pos:self.current_pos+B*T]\n",
    "        y = self.tokens[self.current_pos+1:self.current_pos+B*T+1]\n",
    "        x = x.view(B, T)\n",
    "        y = y.view(B, T)\n",
    "        self.current_pos += B * T\n",
    "\n",
    "        if (len(self.tokens) - self.current_pos + 1) < B * T:\n",
    "            self.current_pos = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6078d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.W_pos = nn.Embedding(config.context_len, config.embedding_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # Don't convert to tensor if already a tensor\n",
    "        if not isinstance(tokens, torch.Tensor):\n",
    "            tokens = torch.tensor(tokens)\n",
    "        \n",
    "        embeddings = self.W_E(tokens)\n",
    "\n",
    "        # Create positions tensor on the same device as tokens\n",
    "        positions = torch.arange(tokens.shape[1], device=tokens.device)\n",
    "        position_embeddings = self.W_pos(positions)\n",
    "\n",
    "        return embeddings + position_embeddings\n",
    "\n",
    "class DeEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.W_D = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.W_D(x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "42f447d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_Q = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_K = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        self.W_V = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "        \n",
    "        self.W_out = nn.Linear(config.embedding_dim, config.embedding_dim, bias=False)\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attention_dim = config.attention_dim\n",
    "        \n",
    "        # Register causal mask as buffer\n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.context_len, config.context_len)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        Q = einops.rearrange(self.W_Q(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        K = einops.rearrange(self.W_K(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "        V = einops.rearrange(self.W_V(x), 'batch seq (head dim) -> batch head seq dim', head=self.num_heads)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        scores = (Q @ K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.attention_dim))\n",
    "        \n",
    "        # Apply causal mask\n",
    "        scores = scores.masked_fill(self.causal_mask[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        QK = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        QKV = einops.rearrange(QK @ V, 'batch head seq dim -> batch seq (head dim)', head=self.num_heads)\n",
    "        QKV_Out = self.W_out(QKV)\n",
    "\n",
    "        return QKV_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2142239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(config.embedding_dim, config.mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(config.mlp_dim, config.embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer2(self.gelu(self.layer1(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "154e6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Attention_Layers = Attention(config)\n",
    "        self.MLP_Layers = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.Attention_Layers(x)\n",
    "        return x + self.MLP_Layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d97092ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = Embedding(config)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for i in range(config.num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.deembed = DeEmbedding(config)\n",
    "        self.deembed.W_D.weight = self.embed.W_E.weight  # tie weights\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        # print(f\"after embed: {x}\")\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.deembed(x)\n",
    "        # print(f\"after deembed: {x}\")\n",
    "        x = torch.softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2db70dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(inference_config, inference_model, text=\"They fear us\"):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    x = torch.tensor(tokens)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    \n",
    "    # Move input tensor to the same device as the model\n",
    "    if inference_model:\n",
    "        print(\"using passed in model for inference\")\n",
    "        device = next(inference_model.parameters()).device\n",
    "\n",
    "    else:\n",
    "        inference_model = Transformer(inference_config)\n",
    "        print(\"using random model for inference\")\n",
    "        device = inference_config.device\n",
    "    \n",
    "    x = x.to(device)\n",
    "    out = inference_model(x)\n",
    "    pred_tokens = out.argmax(dim=-1)\n",
    "    print(f\"predicted tokens: {pred_tokens}\")\n",
    "\n",
    "    # Only take the prediction from the last position (next token after \"fox\")\n",
    "    next_token = pred_tokens[0, -1].item()\n",
    "    predicted_word = tokenizer.decode([next_token])\n",
    "    print(f\"predicted word: {predicted_word}\")\n",
    "\n",
    "    print(f\"full sentence: {text}{predicted_word}\")\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    print(\"sanity check: all predicted tokens\")\n",
    "    for num, token in enumerate(pred_tokens.flatten()):\n",
    "        decoded = tokenizer.decode([token.item()])\n",
    "        \n",
    "        if num == (len(pred_tokens.flatten()) - 1):\n",
    "            print(f\"** Token {token} -> '{decoded}' **\")\n",
    "        else:\n",
    "            print(f\"Token {token} -> '{decoded}'\")\n",
    "\n",
    "def training(model_config):\n",
    "    device = model_config.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    if config.wandb_enabled:\n",
    "        # Initialize wandb\n",
    "        wandb.init(\n",
    "            project=os.getenv(\"WANDB_PROJECT\"),\n",
    "            entity=os.getenv(\"WANDB_ENTITY\"),\n",
    "            config={\n",
    "                \"vocab_size\": model_config.vocab_size,\n",
    "                \"embedding_dim\": model_config.embedding_dim,\n",
    "                \"mlp_dim\": model_config.mlp_dim,\n",
    "                \"num_blocks\": model_config.num_blocks,\n",
    "                \"num_heads\": model_config.num_heads,\n",
    "                \"context_len\": model_config.context_len,\n",
    "                \"attention_dim\": model_config.attention_dim,\n",
    "                \"device\": model_config.device,\n",
    "                \"num_epochs\": model_config.num_epochs,\n",
    "                \"lr\": model_config.lr,\n",
    "                \"betas\": model_config.betas,\n",
    "                \"eps\": model_config.eps,\n",
    "                \"weight_decay\": model_config.weight_decay,\n",
    "                \"use_muon\": model_config.use_muon,\n",
    "                \"muon_lr\": model_config.muon_lr,\n",
    "                \"muon_momentum\": model_config.muon_momentum,\n",
    "                \"learnable_params\": model_config.learnable_params,\n",
    "                \"total_params\": model_config.learnable_params + model_config.non_learnable_params\n",
    "            }\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(8, 1024)\n",
    "\n",
    "    model = Transformer(model_config).to(device)\n",
    "    # model = torch.compile(model) # temporary comment to resolve errors with metal\n",
    "    losses = []\n",
    "    \n",
    "    # Create optimizer based on config\n",
    "    if model_config.use_muon and MUON_AVAILABLE:\n",
    "        print(\"Using Muon optimizer for hidden layers\")\n",
    "        \n",
    "        # Separate parameters for Muon optimization\n",
    "        hidden_weights = []\n",
    "        other_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            # Hidden weights are 2D+ parameters in transformer blocks\n",
    "            if param.ndim >= 2 and any(block_name in name for block_name in ['blocks',\n",
    "        'Attention_Layers', 'MLP_Layers']):\n",
    "                hidden_weights.append(param)\n",
    "                if model_config.print_per_layer_params:\n",
    "                    print(f\"  MUON: {name} - shape: {param.shape} - params: {param.numel():,}\")\n",
    "            else:\n",
    "                # Embeddings, biases, and other parameters\n",
    "                other_params.append(param)\n",
    "                if model_config.print_per_layer_params:\n",
    "                    print(f\"  ADAMW: {name} - shape: {param.shape} - params: {param.numel():,}\")\n",
    "\n",
    "        print(f\"Muon params: {sum(p.numel() for p in hidden_weights):,}\")\n",
    "        print(f\"AdamW params: {sum(p.numel() for p in other_params):,}\")\n",
    "        print(f\"Total: {model_config.non_learnable_params + sum(p.numel() for p in hidden_weights) + sum(p.numel() for p in \n",
    "        other_params):,}\")\n",
    "\n",
    "        print(\"*\"*50)\n",
    "        \n",
    "        param_groups = [\n",
    "            {\n",
    "                'params': hidden_weights, \n",
    "                'use_muon': True, \n",
    "                'lr': model_config.muon_lr, \n",
    "                'weight_decay': model_config.weight_decay,\n",
    "                'momentum': model_config.muon_momentum\n",
    "            },\n",
    "            {\n",
    "                'params': other_params, \n",
    "                'use_muon': False, \n",
    "                'lr': model_config.lr,\n",
    "                'weight_decay': model_config.weight_decay,\n",
    "                'betas': model_config.betas,\n",
    "                'eps': model_config.eps\n",
    "            }\n",
    "        ]\n",
    "        optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
    "    else:\n",
    "        if model_config.use_muon and not MUON_AVAILABLE:\n",
    "            print(\"Muon requested but not available, falling back to AdamW\")\n",
    "        else:\n",
    "            print(\"Using AdamW optimizer\")\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=model_config.lr, \n",
    "                                     betas=model_config.betas, eps=model_config.eps, \n",
    "                                     weight_decay=model_config.weight_decay)\n",
    "\n",
    "    for epoch in range(model_config.num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{model_config.num_epochs}\")\n",
    "\n",
    "        # Reset data loader position at the start of each epoch\n",
    "        train_loader.current_pos = 0\n",
    "\n",
    "        num_batches = int(len(train_loader.tokens) / (train_loader.batch_size * train_loader.seq_len))\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            x, y = train_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if config.wandb_enabled:\n",
    "                # Log to wandb\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"batch\": batch,\n",
    "                    \"step\": epoch * num_batches + batch\n",
    "                })\n",
    "\n",
    "            if batch % 1 == 0:\n",
    "                print(f\"Batch {batch}/{num_batches}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if config.wandb_enabled:\n",
    "        wandb.finish()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d20605a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnable params dict: {'embedding': 36185040, 'positional_embedding': 737280, 'MLPs (Weights)': 4147200, 'MLPs (Biases)': 4320, 'W_Qs': 1036800, 'W_Ks': 1036800, 'W_Vs': 1036800, 'W_Out': 1036800}\n",
      "total # of learnable params: 45,221,040\n",
      "non-learnable params dict: {'deembedding (tied to embedding weights)': 36185040}\n",
      "total # of non-learnable params: 36,185,040\n",
      "** total # of params: 81,406,080 **\n",
      "**************************************************\n",
      "theoretical start loss: 10.82490511970208\n",
      "Using device: mps\n",
      "loaded 338025 tokens with batch size of 8 sequences and 1024 tokens per sequence in the batch\n",
      "each epoch has 41.2628173828125 batches, with 8192 tokens per batch, for a total of 338025.0 tokens\n",
      "**************************************************\n",
      "Using Muon optimizer for hidden layers\n",
      "Muon params: 8,294,400\n",
      "AdamW params: 36,926,640\n",
      "Total: 81,406,080\n",
      "**************************************************\n",
      "Epoch 1/1\n",
      "Batch 0/41, Loss: 10.8006\n",
      "Batch 1/41, Loss: 10.8043\n",
      "Batch 2/41, Loss: 10.7966\n",
      "Batch 3/41, Loss: 10.7992\n",
      "Batch 4/41, Loss: 10.8020\n",
      "Batch 5/41, Loss: 10.8076\n",
      "Batch 6/41, Loss: 10.8030\n",
      "Batch 7/41, Loss: 10.8025\n",
      "Batch 8/41, Loss: 10.7965\n",
      "Batch 9/41, Loss: 10.8046\n",
      "Batch 10/41, Loss: 10.8028\n",
      "Batch 11/41, Loss: 10.8074\n",
      "Batch 12/41, Loss: 10.8113\n",
      "Batch 13/41, Loss: 10.8091\n",
      "Batch 14/41, Loss: 10.8107\n",
      "Batch 15/41, Loss: 10.8066\n",
      "Batch 16/41, Loss: 10.8008\n",
      "Batch 17/41, Loss: 10.8052\n",
      "Batch 18/41, Loss: 10.8028\n",
      "Batch 19/41, Loss: 10.8068\n",
      "Batch 20/41, Loss: 10.8065\n",
      "Batch 21/41, Loss: 10.8003\n",
      "Batch 22/41, Loss: 10.8118\n",
      "Batch 23/41, Loss: 10.8020\n",
      "Batch 24/41, Loss: 10.8020\n",
      "Batch 25/41, Loss: 10.8058\n",
      "Batch 26/41, Loss: 10.8066\n",
      "Batch 27/41, Loss: 10.8072\n",
      "Batch 28/41, Loss: 10.8097\n",
      "Batch 29/41, Loss: 10.8021\n",
      "Batch 30/41, Loss: 10.8072\n",
      "Batch 31/41, Loss: 10.7972\n",
      "Batch 32/41, Loss: 10.8000\n",
      "Batch 33/41, Loss: 10.8024\n",
      "Batch 34/41, Loss: 10.8017\n",
      "Batch 35/41, Loss: 10.8039\n",
      "Batch 36/41, Loss: 10.8042\n",
      "Batch 37/41, Loss: 10.8016\n",
      "Batch 38/41, Loss: 10.7983\n",
      "Batch 39/41, Loss: 10.7952\n",
      "Batch 40/41, Loss: 10.8014\n",
      "**************************************************\n",
      "using passed in model for inference\n",
      "predicted tokens: tensor([[2990, 3252,  514]], device='mps:0')\n",
      "predicted word:  us\n",
      "full sentence: They fear us us\n",
      "**************************************************\n",
      "sanity check: all predicted tokens\n",
      "Token 2990 -> 'They'\n",
      "Token 3252 -> ' fear'\n",
      "** Token 514 -> ' us' **\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = Config(wandb_enabled=False)\n",
    "    config.display_config(extended=True)\n",
    "    print(f\"theoretical start loss: {np.log(config.vocab_size)}\")\n",
    "    model = training(config)\n",
    "    inference(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6fb752e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "using passed in model for inference\n",
      "predicted tokens: tensor([[  464, 48718,  1992,   318]], device='mps:0')\n",
      "predicted word:  is\n",
      "full sentence: The french President is is\n",
      "**************************************************\n",
      "sanity check: all predicted tokens\n",
      "Token 464 -> 'The'\n",
      "Token 48718 -> ' french'\n",
      "Token 1992 -> ' President'\n",
      "** Token 318 -> ' is' **\n"
     ]
    }
   ],
   "source": [
    "inference(config, model, text=\"The french President is\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
